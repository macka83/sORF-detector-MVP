{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d79d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pliki CSV znalezione. Gotowy do pracy.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycdhit import cd_hit, read_clstr\n",
    "\n",
    "# Konfiguracja nazw plików\n",
    "POSITIVES_FILE = \"positives_filtered_with_upstream.csv\"\n",
    "NEGATIVES_FILE = \"negatives_diverse_13k.csv\"\n",
    "\n",
    "# Upewnij się, że pliki istnieją\n",
    "if not os.path.exists(POSITIVES_FILE) or not os.path.exists(NEGATIVES_FILE):\n",
    "    print(\"BŁĄD: Nie znaleziono plików CSV w katalogu roboczym!\")\n",
    "else:\n",
    "    print(\"Pliki CSV znalezione. Gotowy do pracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7789af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcolisORFDetector:\n",
    "    \"\"\"\n",
    "    Zoptymalizowany detektor sORF dla E. coli.\n",
    "    Zawiera poprawioną obsługę CD-HIT i upstream_30bp.\n",
    "    \"\"\"\n",
    "\n",
    "    SHINE_DALGARNO = [\"AGGAGG\", \"GGAGG\", \"GAGG\", \"GGAG\", \"GGA\"]\n",
    "    ECOLI_START_CODONS = {\"ATG\": 1.0, \"GTG\": 0.52, \"TTG\": 0.28, \"CTG\": 0.15}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=42\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def shine_dalgarno_score(self, upstream_seq):\n",
    "        \"\"\"Oblicza wynik dopasowania Shine-Dalgarno\"\"\"\n",
    "        if pd.isna(upstream_seq) or len(str(upstream_seq)) < 5:\n",
    "            return 0.0\n",
    "\n",
    "        upstream_seq = str(upstream_seq).upper()\n",
    "        rbs_score = 0\n",
    "        u_len = len(upstream_seq)\n",
    "\n",
    "        for rbs in self.SHINE_DALGARNO:\n",
    "            pos = upstream_seq.rfind(rbs)\n",
    "            if pos != -1:\n",
    "                # Dystans od końca RBS do końca sekwencji upstream (czyli do startu sORF)\n",
    "                distance = u_len - (pos + len(rbs))\n",
    "                # Optymalny dystans u E. coli to ok. 5-12 pz\n",
    "                if 5 <= distance <= 13:\n",
    "                    score = (13 - distance) / 8.0  # Heurystyka punktacji\n",
    "                    rbs_score = max(rbs_score, score)\n",
    "        return rbs_score\n",
    "\n",
    "    def extract_features(self, sorf_seq, upstream_seq):\n",
    "        \"\"\"Ekstrakcja cech numerycznych\"\"\"\n",
    "        features = {}\n",
    "        seq = str(sorf_seq).upper()\n",
    "        u_seq = str(upstream_seq).upper() if pd.notna(upstream_seq) else \"\"\n",
    "\n",
    "        # 1. RBS Score (Kluczowe dla E. coli)\n",
    "        features[\"rbs_score\"] = self.shine_dalgarno_score(u_seq)\n",
    "\n",
    "        # 2. Start Codon Strength\n",
    "        start_codon = seq[:3]\n",
    "        features[\"start_strength\"] = self.ECOLI_START_CODONS.get(start_codon, 0.0)\n",
    "\n",
    "        # 3. Parametry podstawowe\n",
    "        features[\"length_norm\"] = min(len(seq) / 300.0, 1.0)\n",
    "        features[\"gc_content\"] = (\n",
    "            (seq.count(\"G\") + seq.count(\"C\")) / len(seq) if len(seq) > 0 else 0\n",
    "        )\n",
    "        features[\"at_bias\"] = (\n",
    "            (seq.count(\"A\") + seq.count(\"T\")) / len(seq) if len(seq) > 0 else 0\n",
    "        )\n",
    "\n",
    "        # 4. Prosty indeks stabilności (np. GC upstream)\n",
    "        features[\"upstream_gc\"] = (\n",
    "            (u_seq.count(\"G\") + u_seq.count(\"C\")) / len(u_seq) if len(u_seq) > 0 else 0\n",
    "        )\n",
    "\n",
    "        return list(features.values())\n",
    "\n",
    "    def cluster_sequences(self, fasta_file, output_prefix, threshold=0.9):\n",
    "        \"\"\"\n",
    "        Uruchamia CD-HIT i zwraca listę ID reprezentantów.\n",
    "        Naprawiony błąd parsowania pliku .clstr\n",
    "        \"\"\"\n",
    "        # Uruchomienie CD-HIT (zapisuje pliki na dysku)\n",
    "        # c=threshold, G=1 (global), d=0 (pełny nagłówek)\n",
    "        try:\n",
    "            cd_hit(i=fasta_file, o=output_prefix, c=threshold, d=0, sc=1, G=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Ostrzeżenie CD-HIT (może być problem ze ścieżką): {e}\")\n",
    "            # Sprawdź czy plik wyjściowy powstał mimo błędu wrappera\n",
    "            if not os.path.exists(output_prefix + \".clstr\"):\n",
    "                raise e\n",
    "\n",
    "        # Ręczne parsowanie pliku .clstr (niezawodne)\n",
    "        clstr_file = f\"{output_prefix}.clstr\"\n",
    "        representatives = []\n",
    "\n",
    "        with open(clstr_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                # Szukamy linii z gwiazdką '*', która oznacza reprezentanta klastra\n",
    "                # Format: 0\t297nt, >seq_123... *\n",
    "                if \"*\" in line:\n",
    "                    # Wyciągamy ID sekwencji (pomiędzy > a ...)\n",
    "                    match = re.search(r\">(.+?)\\.\\.\\.\", line)\n",
    "                    if match:\n",
    "                        representatives.append(match.group(1))\n",
    "                    else:\n",
    "                        # Fallback dla krótkich nagłówków bez kropek\n",
    "                        match_simple = re.search(r\">(.+?)\\s\", line) or re.search(\n",
    "                            r\">(.+?)$\", line\n",
    "                        )\n",
    "                        if match_simple:\n",
    "                            # Usuwamy ewentualny znak * na końcu jeśli złapał się regexem\n",
    "                            clean_id = match_simple.group(1).replace(\"*\", \"\").strip()\n",
    "                            representatives.append(clean_id)\n",
    "\n",
    "        return representatives\n",
    "\n",
    "\n",
    "def df_to_fasta(df, filename, seq_col=\"sorf\"):\n",
    "    \"\"\"Pomocnicza funkcja zapisu do FASTA\"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for idx, row in df.iterrows():\n",
    "            # Używamy indeksu jako ID, żeby łatwo filtrować\n",
    "            f.write(f\">seq_{idx}\\n{row[seq_col]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038e970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie danych...\n",
      "Liczba surowych pozytywów: 13570\n",
      "Liczba surowych negatywów: 13000\n",
      "\n",
      "Uruchamianie CD-HIT dla pozytywów...\n",
      "Pozytywy po klastrowaniu (unikalne): 1843\n",
      "Negatywy po balansowaniu: 1843\n",
      "\n",
      "FINAŁOWY ZBIÓR: 1843 sORF vs 1843 Non-sORF\n"
     ]
    }
   ],
   "source": [
    "# 1. Wczytanie danych\n",
    "print(\"Wczytywanie danych...\")\n",
    "pos_df = pd.read_csv(POSITIVES_FILE)\n",
    "neg_df = pd.read_csv(NEGATIVES_FILE)\n",
    "\n",
    "print(f\"Liczba surowych pozytywów: {len(pos_df)}\")\n",
    "print(f\"Liczba surowych negatywów: {len(neg_df)}\")\n",
    "\n",
    "# 2. Klastrowanie Pozytywów (Usuwanie duplikatów)\n",
    "print(\"\\nUruchamianie CD-HIT dla pozytywów...\")\n",
    "df_to_fasta(pos_df, \"positives.fasta\", seq_col=\"sorf\")\n",
    "\n",
    "detector = EcolisORFDetector()\n",
    "# Próg 0.9 (90% identyczności) usuwa bardzo podobne sekwencje\n",
    "keep_ids = detector.cluster_sequences(\n",
    "    \"positives.fasta\", \"positives_clustered\", threshold=0.9\n",
    ")\n",
    "\n",
    "# Parsowanie ID z powrotem na indeksy (format: seq_123 -> 123)\n",
    "keep_indices = [int(x.replace(\"seq_\", \"\")) for x in keep_ids if \"seq_\" in x]\n",
    "\n",
    "# Filtrowanie DataFrame\n",
    "pos_filtered = pos_df.loc[keep_indices].copy()\n",
    "print(f\"Pozytywy po klastrowaniu (unikalne): {len(pos_filtered)}\")\n",
    "\n",
    "# 3. Balansowanie klas (Undersampling)\n",
    "# Chcemy tyle samo negatywów, co unikalnych pozytywów\n",
    "n_samples = len(pos_filtered)\n",
    "\n",
    "if len(neg_df) > n_samples:\n",
    "    neg_balanced = neg_df.sample(n_samples, random_state=42).copy()\n",
    "else:\n",
    "    neg_balanced = neg_df.copy()  # Jeśli negatywów jest mniej, bierzemy wszystkie\n",
    "\n",
    "print(f\"Negatywy po balansowaniu: {len(neg_balanced)}\")\n",
    "\n",
    "# Sprawdzenie końcowe\n",
    "print(f\"\\nFINAŁOWY ZBIÓR: {len(pos_filtered)} sORF vs {len(neg_balanced)} Non-sORF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f72ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generowanie cech...\n",
      "Rozmiar zbioru treningowego: (2948, 6)\n",
      "Rozmiar zbioru testowego: (738, 6)\n",
      "\n",
      "Trenowanie klasyfikatora Random Forest...\n",
      "\n",
      "==================================================\n",
      "WYNIKI KLASYFIKACJI sORF (E. coli)\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-sORF       0.88      0.93      0.90       369\n",
      "        sORF       0.93      0.87      0.90       369\n",
      "\n",
      "    accuracy                           0.90       738\n",
      "   macro avg       0.90      0.90      0.90       738\n",
      "weighted avg       0.90      0.90      0.90       738\n",
      "\n",
      "ROC AUC Score: 0.9712\n",
      "\n",
      "Najważniejsze cechy:\n",
      "1. length_norm: 0.5240\n",
      "2. rbs_score: 0.1830\n",
      "3. start_strength: 0.1050\n",
      "4. upstream_gc: 0.0680\n",
      "5. at_bias: 0.0609\n",
      "6. gc_content: 0.0590\n"
     ]
    }
   ],
   "source": [
    "# 4. Generowanie macierzy X, y\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(\"\\nGenerowanie cech...\")\n",
    "\n",
    "# Przetwarzanie pozytywów\n",
    "# Używamy kolumny 'upstream_30bp' zgodnie z Twoimi plikami\n",
    "for _, row in pos_filtered.iterrows():\n",
    "    feats = detector.extract_features(row[\"sorf\"], row.get(\"upstream_30bp\", \"\"))\n",
    "    X.append(feats)\n",
    "    y.append(1)\n",
    "\n",
    "# Przetwarzanie negatywów\n",
    "for _, row in neg_balanced.iterrows():\n",
    "    feats = detector.extract_features(row[\"sorf\"], row.get(\"upstream_30bp\", \"\"))\n",
    "    X.append(feats)\n",
    "    y.append(0)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 5. Podział na zbiór treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test.shape}\")\n",
    "\n",
    "# 6. Trening modelu\n",
    "print(\"\\nTrenowanie klasyfikatora Random Forest...\")\n",
    "detector.scaler.fit(X_train)\n",
    "X_train_scaled = detector.scaler.transform(X_train)\n",
    "detector.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Ewaluacja\n",
    "X_test_scaled = detector.scaler.transform(X_test)\n",
    "y_pred = detector.model.predict(X_test_scaled)\n",
    "y_probs = detector.model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"WYNIKI KLASYFIKACJI sORF (E. coli)\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Non-sORF\", \"sORF\"]))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_probs):.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\n",
    "    \"rbs_score\",\n",
    "    \"start_strength\",\n",
    "    \"length_norm\",\n",
    "    \"gc_content\",\n",
    "    \"at_bias\",\n",
    "    \"upstream_gc\",\n",
    "]\n",
    "importances = detector.model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nNajważniejsze cechy:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535f937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
