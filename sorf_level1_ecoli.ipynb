{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d79d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycdhit import cd_hit, read_clstr\n",
    "from remove_duplicates_from_negatives import (\n",
    "    duplicates_analysis,\n",
    "    duplicates_removal,\n",
    "    balance_negatives,\n",
    "    verify_results,\n",
    ")\n",
    "\n",
    "# Konfiguracja nazw plików\n",
    "# POSITIVES_FILE = \"positives_filtered_with_upstream.csv\"\n",
    "# NEGATIVES_FILE = \"negatives_diverse_13k.csv\"\n",
    "\n",
    "POSITIVES_FILE = \"./data/positives_with_upstream.csv\"\n",
    "NEGATIVES_FILE = \"./data/negatives_diverse_100k.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7789af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcolisORFDetector:\n",
    "    \"\"\"\n",
    "    Zoptymalizowany detektor sORF dla E. coli.\n",
    "    Zawiera poprawioną obsługę CD-HIT i upstream_30bp.\n",
    "    \"\"\"\n",
    "\n",
    "    SHINE_DALGARNO = [\"AGGAGG\", \"GGAGG\", \"GAGG\", \"GGAG\", \"GGA\"]\n",
    "    ECOLI_START_CODONS = {\"ATG\": 1.0, \"GTG\": 0.52, \"TTG\": 0.28, \"CTG\": 0.15}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=42\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def shine_dalgarno_score(self, upstream_seq):\n",
    "        \"\"\"Oblicza wynik dopasowania Shine-Dalgarno\"\"\"\n",
    "        if pd.isna(upstream_seq) or len(str(upstream_seq)) < 5:\n",
    "            return 0.0\n",
    "\n",
    "        upstream_seq = str(upstream_seq).upper()\n",
    "        rbs_score = 0\n",
    "        u_len = len(upstream_seq)\n",
    "\n",
    "        for rbs in self.SHINE_DALGARNO:\n",
    "            pos = upstream_seq.rfind(rbs)\n",
    "            if pos != -1:\n",
    "                # Dystans od końca RBS do końca sekwencji upstream (czyli do startu sORF)\n",
    "                distance = u_len - (pos + len(rbs))\n",
    "                # Optymalny dystans u E. coli to ok. 5-12 pz\n",
    "                if 5 <= distance <= 13:\n",
    "                    score = (13 - distance) / 8.0  # Heurystyka punktacji\n",
    "                    rbs_score = max(rbs_score, score)\n",
    "        return rbs_score\n",
    "\n",
    "    def extract_features(self, sorf_seq, upstream_seq):\n",
    "        \"\"\"Ekstrakcja cech numerycznych\"\"\"\n",
    "        features = {}\n",
    "        seq = str(sorf_seq).upper()\n",
    "        u_seq = str(upstream_seq).upper() if pd.notna(upstream_seq) else \"\"\n",
    "\n",
    "        # 1. RBS Score (Kluczowe dla E. coli)\n",
    "        features[\"rbs_score\"] = self.shine_dalgarno_score(u_seq)\n",
    "\n",
    "        # 2. Start Codon Strength\n",
    "        start_codon = seq[:3]\n",
    "        features[\"start_strength\"] = self.ECOLI_START_CODONS.get(start_codon, 0.0)\n",
    "\n",
    "        # 3. Parametry podstawowe\n",
    "        features[\"length_norm\"] = min(len(seq) / 300.0, 1.0)\n",
    "        features[\"gc_content\"] = (\n",
    "            (seq.count(\"G\") + seq.count(\"C\")) / len(seq) if len(seq) > 0 else 0\n",
    "        )\n",
    "        features[\"at_bias\"] = (\n",
    "            (seq.count(\"A\") + seq.count(\"T\")) / len(seq) if len(seq) > 0 else 0\n",
    "        )\n",
    "\n",
    "        # 4. Prosty indeks stabilności (np. GC upstream)\n",
    "        features[\"upstream_gc\"] = (\n",
    "            (u_seq.count(\"G\") + u_seq.count(\"C\")) / len(u_seq) if len(u_seq) > 0 else 0\n",
    "        )\n",
    "\n",
    "        return list(features.values())\n",
    "\n",
    "    def cluster_sequences(self, fasta_file, output_prefix, threshold=0.9):\n",
    "        \"\"\"\n",
    "        Uruchamia CD-HIT i zwraca listę ID reprezentantów.\n",
    "        Naprawiony błąd parsowania pliku .clstr\n",
    "        \"\"\"\n",
    "        # Uruchomienie CD-HIT (zapisuje pliki na dysku)\n",
    "        # c=threshold, G=1 (global), d=0 (pełny nagłówek)\n",
    "        try:\n",
    "            cd_hit(i=fasta_file, o=output_prefix, c=threshold, d=0, sc=1, G=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Ostrzeżenie CD-HIT (może być problem ze ścieżką): {e}\")\n",
    "            # Sprawdź czy plik wyjściowy powstał mimo błędu wrappera\n",
    "            if not os.path.exists(output_prefix + \".clstr\"):\n",
    "                raise e\n",
    "\n",
    "        # Ręczne parsowanie pliku .clstr (niezawodne)\n",
    "        clstr_file = f\"{output_prefix}.clstr\"\n",
    "        representatives = []\n",
    "\n",
    "        with open(clstr_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                # Szukamy linii z gwiazdką '*', która oznacza reprezentanta klastra\n",
    "                # Format: 0\t297nt, >seq_123... *\n",
    "                if \"*\" in line:\n",
    "                    # Wyciągamy ID sekwencji (pomiędzy > a ...)\n",
    "                    match = re.search(r\">(.+?)\\.\\.\\.\", line)\n",
    "                    if match:\n",
    "                        representatives.append(match.group(1))\n",
    "                    else:\n",
    "                        # Fallback dla krótkich nagłówków bez kropek\n",
    "                        match_simple = re.search(r\">(.+?)\\s\", line) or re.search(\n",
    "                            r\">(.+?)$\", line\n",
    "                        )\n",
    "                        if match_simple:\n",
    "                            # Usuwamy ewentualny znak * na końcu jeśli złapał się regexem\n",
    "                            clean_id = match_simple.group(1).replace(\"*\", \"\").strip()\n",
    "                            representatives.append(clean_id)\n",
    "\n",
    "        return representatives\n",
    "\n",
    "\n",
    "def df_to_fasta(df, filename, seq_col=\"sorf\"):\n",
    "    \"\"\"Pomocnicza funkcja zapisu do FASTA\"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for idx, row in df.iterrows():\n",
    "            # Używamy indeksu jako ID, żeby łatwo filtrować\n",
    "            f.write(f\">seq_{idx}\\n{row[seq_col]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038e970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie danych...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30350/1952969864.py:3: DtypeWarning: Columns (13,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pos_df = pd.read_csv(POSITIVES_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba surowych pozytywów: 100000\n",
      "Liczba surowych negatywów: 98347\n",
      "\n",
      "Uruchamianie CD-HIT dla pozytywów...\n",
      "Pozytywy po klastrowaniu (unikalne): 3818\n",
      "Liczba duplikatów (dokładnych powtórzeń sekwencji): 16,638\n",
      "Procent duplikatów: 16.92%\n",
      "\n",
      "Top 10 najczęściej powtarzanych sekwencji sORF:\n",
      "   1. Sekwencja #ATGAATAAAC...TGA   pojawia się    44 razy\n",
      "   2. Sekwencja #ATGCTGTATT...TAA   pojawia się    41 razy\n",
      "   3. Sekwencja #ATGCTGAACT...TAG   pojawia się    38 razy\n",
      "   4. Sekwencja #ATGACTCTAG...TAA   pojawia się    35 razy\n",
      "   5. Sekwencja #ATGCTACGCG...TAA   pojawia się    35 razy\n",
      "   6. Sekwencja #ATGCGAAAAC...TAG   pojawia się    35 razy\n",
      "   7. Sekwencja #ATGTAACGAA...TAG   pojawia się    35 razy\n",
      "   8. Sekwencja #ATGAGTGAAA...TAG   pojawia się    35 razy\n",
      "   9. Sekwencja #ATGTTGAAGC...TGA   pojawia się    35 razy\n",
      "   10. Sekwencja #ATGATTCGTC...TAG   pojawia się    35 razy\n",
      "\n",
      "Statystyki długości sekwencji:\n",
      "   Min: 30 bp, Max: 198 bp, Średnia: 90.4 bp\n",
      "✓ Usunięto duplikaty. Nowy rozmiar zbioru: 81,709\n",
      "✓ Oszczędzono 83.1% oryginalnych danych (unikalne reprezentanty).\n",
      "✓ Pomyślnie wylosowano 3818 unikalnych negatywów.\n",
      "   (Stosunek 1:1 do pozytywów)\n",
      "✓ Wczytano zmienną 'neg_df'. Rozmiar oryginalny: 98,347 próbek\n",
      "✓ Cel balansowania (liczba unikalnych pozytywów): 3818\n",
      "Negatywy po balansowaniu: 3818\n",
      "\n",
      "FINAŁOWY ZBIÓR: 3818 sORF vs 3818 Non-sORF\n"
     ]
    }
   ],
   "source": [
    "# 1. Wczytanie danych\n",
    "print(\"Wczytywanie danych...\")\n",
    "pos_df = pd.read_csv(POSITIVES_FILE)\n",
    "neg_df = pd.read_csv(NEGATIVES_FILE)\n",
    "\n",
    "print(f\"Liczba surowych pozytywów: {len(pos_df)}\")\n",
    "print(f\"Liczba surowych negatywów: {len(neg_df)}\")\n",
    "\n",
    "# 2. Klastrowanie Pozytywów (Usuwanie duplikatów)\n",
    "print(\"\\nUruchamianie CD-HIT dla pozytywów...\")\n",
    "df_to_fasta(pos_df, \"positives.fasta\", seq_col=\"sorf\")\n",
    "\n",
    "detector = EcolisORFDetector()\n",
    "# Próg 0.9 (90% identyczności) usuwa bardzo podobne sekwencje\n",
    "keep_ids = detector.cluster_sequences(\n",
    "    \"positives.fasta\", \"positives_clustered\", threshold=0.9\n",
    ")\n",
    "\n",
    "# Parsowanie ID z powrotem na indeksy (format: seq_123 -> 123)\n",
    "keep_indices = [int(x.replace(\"seq_\", \"\")) for x in keep_ids if \"seq_\" in x]\n",
    "\n",
    "# Filtrowanie DataFrame\n",
    "pos_filtered = pos_df.loc[keep_indices].copy()\n",
    "print(f\"Pozytywy po klastrowaniu (unikalne): {len(pos_filtered)}\")\n",
    "\n",
    "# 3. Balansowanie klas (Undersampling)\n",
    "# Chcemy tyle samo negatywów, co unikalnych pozytywów\n",
    "# Sprawdzenie obecności zmiennych\n",
    "if \"neg_df\" not in locals():\n",
    "    raise ValueError(\n",
    "        \"BŁĄD: Zmienna 'neg_df' nie istnieje! Wczytaj najpierw dane negatywne.\"\n",
    "    )\n",
    "if \"pos_filtered\" not in locals():\n",
    "    print(\"⚠️ Ostrzeżenie: Brak 'pos_filtered'\")\n",
    "else:\n",
    "    target_n = len(pos_filtered)\n",
    "\n",
    "duplicates_analysis(neg_df)\n",
    "neg_df_unique = duplicates_removal(neg_df)\n",
    "neg_balanced = balance_negatives(neg_df_unique, target_n)\n",
    "\n",
    "\n",
    "print(f\"✓ Wczytano zmienną 'neg_df'. Rozmiar oryginalny: {len(neg_df):,} próbek\")\n",
    "print(f\"✓ Cel balansowania (liczba unikalnych pozytywów): {target_n}\")\n",
    "\n",
    "print(f\"Negatywy po balansowaniu: {len(neg_balanced)}\")\n",
    "\n",
    "# Sprawdzenie końcowe\n",
    "print(f\"\\nFINAŁOWY ZBIÓR: {len(pos_filtered)} sORF vs {len(neg_balanced)} Non-sORF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f72ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generowanie cech...\n",
      "Rozmiar zbioru treningowego: (6108, 6)\n",
      "Rozmiar zbioru testowego: (1528, 6)\n",
      "\n",
      "Trenowanie klasyfikatora Random Forest...\n",
      "\n",
      "==================================================\n",
      "WYNIKI KLASYFIKACJI sORF (E. coli)\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-sORF       0.95      0.96      0.95       764\n",
      "        sORF       0.96      0.95      0.95       764\n",
      "\n",
      "    accuracy                           0.95      1528\n",
      "   macro avg       0.95      0.95      0.95      1528\n",
      "weighted avg       0.95      0.95      0.95      1528\n",
      "\n",
      "ROC AUC Score: 0.9903\n",
      "\n",
      "Najważniejsze cechy:\n",
      "1. length_norm: 0.4681\n",
      "2. upstream_gc: 0.2853\n",
      "3. start_strength: 0.1249\n",
      "4. rbs_score: 0.0596\n",
      "5. at_bias: 0.0333\n",
      "6. gc_content: 0.0289\n"
     ]
    }
   ],
   "source": [
    "# 4. Generowanie macierzy X, y\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(\"\\nGenerowanie cech...\")\n",
    "\n",
    "# Przetwarzanie pozytywów\n",
    "# Używamy kolumny 'upstream_30bp' zgodnie z Twoimi plikami\n",
    "for _, row in pos_filtered.iterrows():\n",
    "    feats = detector.extract_features(row[\"sorf\"], row.get(\"upstream_30bp\", \"\"))\n",
    "    X.append(feats)\n",
    "    y.append(1)\n",
    "\n",
    "# Przetwarzanie negatywów\n",
    "for _, row in neg_balanced.iterrows():\n",
    "    feats = detector.extract_features(row[\"sorf\"], row.get(\"upstream_30bp\", \"\"))\n",
    "    X.append(feats)\n",
    "    y.append(0)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 5. Podział na zbiór treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego: {X_test.shape}\")\n",
    "\n",
    "# 6. Trening modelu\n",
    "print(\"\\nTrenowanie klasyfikatora Random Forest...\")\n",
    "detector.scaler.fit(X_train)\n",
    "X_train_scaled = detector.scaler.transform(X_train)\n",
    "detector.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Ewaluacja\n",
    "X_test_scaled = detector.scaler.transform(X_test)\n",
    "y_pred = detector.model.predict(X_test_scaled)\n",
    "y_probs = detector.model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"WYNIKI KLASYFIKACJI sORF (E. coli)\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Non-sORF\", \"sORF\"]))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_probs):.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\n",
    "    \"rbs_score\",\n",
    "    \"start_strength\",\n",
    "    \"length_norm\",\n",
    "    \"gc_content\",\n",
    "    \"at_bias\",\n",
    "    \"upstream_gc\",\n",
    "]\n",
    "importances = detector.model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nNajważniejsze cechy:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535f937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
